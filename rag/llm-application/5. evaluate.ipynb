{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation의 중요성\n",
    "\n",
    "서비스를 안정적으로 운영하기 위해서 \n",
    "사용자가 정확한 정보를 받을 수 있는지\n",
    "\n",
    "\"할루시네이션\" -> \"환각\"\n",
    "LLM이 잘못된 답변을 생성하는 경우\n",
    "\n",
    "## Dataset -> 도메인 전문가가 작성한 \"정답지\"\n",
    "\n",
    "특정 질문이 들어오면 -> 이런 답변을 해야한다.\n",
    "\n",
    "1 + 1 = 2\n",
    "\n",
    "## 78점\n",
    "\n",
    "1. ChatGPT 작성 -> 도메인 전문가가 아님 -> 정답지가 틀렸을 수 있음\n",
    "2. embedding 문제. embedding효율이 떨어져서 -> retrieval된 문서가 잘못돼서 -> 답변이 잘못되는 경우\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (0.4.38)\n",
      "Requirement already satisfied: openai in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: python-dotenv in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: langchain-openai in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-pinecone in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (0.2.13)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (3.11.4)\n",
      "Requirement already satisfied: packaging>=23.2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (24.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (2.12.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langsmith) (0.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (4.65.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.4 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: pinecone[asyncio]<8.0.0,>=6.0.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-pinecone) (7.3.0)\n",
      "Requirement already satisfied: numpy!=2.0.2,>=1.26.4 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-pinecone) (2.3.4)\n",
      "Requirement already satisfied: simsimd>=5.9.11 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-pinecone) (6.5.3)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (1.33)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (9.1.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: aiohttp>=3.9.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.13.2)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.1)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.8.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pydantic<3,>=1->langsmith) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith) (3.4.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.10.23)\n",
      "Requirement already satisfied: colorama in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.22.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\jwy\\learn-ai-tools\\venv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langsmith openai python-dotenv langchain langchain-openai langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['246a6fe3-f807-41d1-a716-851b88459f46',\n",
       "  '6ac16512-2dbc-429c-8340-577cec5e018d',\n",
       "  '6a537c7e-3908-4c73-8b93-4b7277d3fa35',\n",
       "  '40e586bf-9a56-4f2b-9ae6-9008adbba508',\n",
       "  '4bf5d729-2429-4720-a7bc-b7828ae756de',\n",
       "  '51ac4afd-a734-464c-adad-17b7657b594b',\n",
       "  'cecd3a06-1394-4697-a5fa-77614be53404',\n",
       "  'c5fec504-fa77-4053-a42d-9ffb19aee228',\n",
       "  '59ae2628-1e68-49d1-a7aa-b89a70519cc8',\n",
       "  'e862071f-2d8b-42d5-a2d0-57292c4c6260',\n",
       "  '074d7c0a-d65c-46b6-b3ac-01fab9d9bf67',\n",
       "  'c4e14165-9d5e-4b8f-be6f-681e7ea2f5f2',\n",
       "  '640a8536-3c41-47df-96b5-04adc50adb07',\n",
       "  'e0d4a020-bd88-4e4e-bc46-30bb7e1d195c',\n",
       "  '4e6926c4-f9b4-41fd-9a30-401fa0bb9eeb',\n",
       "  '74eef1a4-15c8-473e-b03c-afed70dc21b0',\n",
       "  'e2d49410-db6f-490d-910e-7fd2ca7af214',\n",
       "  '1350d81b-44a3-4f58-ad55-0f8f8e506568',\n",
       "  '11e4c530-a5c6-49f5-ac4c-a1f5fa66feeb',\n",
       "  '5aed9be8-176d-4c80-a4a5-4aa54f4b6e04'],\n",
       " 'count': 20}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"income_tax_dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input_question\": \"제1조에 따른 소득세법의 목적은 무엇인가요?\"},\n",
    "        {\"input_question\": \"'거주자'는 소득세법에서 어떻게 정의되나요?\"},\n",
    "        {\"input_question\": \"'비거주자'는 소득세법에 따라 어떻게 정의되나요?\"},\n",
    "        {\"input_question\": \"소득세법에 따른 '내국법인'은 누구를 의미하나요?\"},\n",
    "        {\"input_question\": \"소득세법에 따라 소득세를 납부할 의무가 있는 사람은 누구인가요?\"},\n",
    "        {\"input_question\": \"거주자의 과세 범위는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세법에 따라 소득은 어떻게 분류되나요?\"},\n",
    "        {\"input_question\": \"종합소득이란 무엇인가요?\"},\n",
    "        {\"input_question\": \"세금이 면제되는 소득의 종류는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세의 과세기간은 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"거주자의 소득세 납세지는 어디인가요?\"},\n",
    "        {\"input_question\": \"비거주자의 소득세 납세지는 어디인가요?\"},\n",
    "        {\"input_question\": \"납세지가 불분명한 경우 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"원천징수세액의 납세지는 어떻게 결정되나요?\"},\n",
    "        {\"input_question\": \"납세자의 사망 시 납세지는 어떻게 되나요?\"},\n",
    "        {\"input_question\": \"신탁 소득에 대한 납세의 범위는 무엇인가요?\"},\n",
    "        {\"input_question\": \"원천징수 대상 소득은 무엇인가요?\"},\n",
    "        {\"input_question\": \"공동 소유 자산의 양도소득은 어떻게 과세되나요?\"},\n",
    "        {\"input_question\": \"이자 소득의 출처는 무엇인가요?\"},\n",
    "        {\"input_question\": \"소득세법에서 배당소득은 어떻게 정의되나요?\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"output_answer\": \"소득세법의 목적은 소득의 성격과 납세자의 부담능력에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지하는 것입니다.\"},\n",
    "        {\"output_answer\": \"'거주자'는 한국에 주소를 두거나 183일 이상 거소를 둔 개인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"'비거주자'는 거주자가 아닌 개인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"'내국법인'은 법인세법 제2조 제1호에 따른 내국법인을 의미합니다.\"},\n",
    "        {\"output_answer\": \"거주자 및 국내원천소득이 있는 비거주자는 소득세를 납부할 의무가 있습니다.\"},\n",
    "        {\"output_answer\": \"거주자는 법에서 규정한 모든 소득에 대해 과세되며, 비거주자는 국내원천소득에 대해서만 과세됩니다.\"},\n",
    "        {\"output_answer\": \"소득은 종합소득, 퇴직소득, 양도소득으로 분류됩니다.\"},\n",
    "        {\"output_answer\": \"종합소득은 이자소득, 배당소득, 사업소득, 근로소득, 연금소득 및 기타소득을 포함합니다.\"},\n",
    "        {\"output_answer\": \"비과세 소득에는 공익신탁의 이익, 특정 사업소득 및 기타 법에서 정한 특정 소득이 포함됩니다.\"},\n",
    "        {\"output_answer\": \"소득세의 과세기간은 매년 1월 1일부터 12월 31일까지입니다.\"},\n",
    "        {\"output_answer\": \"거주자의 소득세 납세지는 주소지이며, 주소지가 없으면 거소지입니다.\"},\n",
    "        {\"output_answer\": \"비거주자의 소득세 납세지는 국내사업장의 소재지입니다. 국내사업장이 여러 곳인 경우 주된 사업장의 소재지가 납세지가 됩니다.\"},\n",
    "        {\"output_answer\": \"납세지가 불분명한 경우 대통령령으로 정합니다.\"},\n",
    "        {\"output_answer\": \"원천징수세액의 납세지는 원천징수자의 종류와 위치에 따라 결정됩니다.\"},\n",
    "        {\"output_answer\": \"납세자의 사망 시 상속인 또는 납세관리인의 주소지나 거소지가 납세지가 됩니다.\"},\n",
    "        {\"output_answer\": \"신탁 소득에 대한 납세의 범위는 신탁의 수익자가 해당 소득에 대해 납세의무를 집니다.\"},\n",
    "        {\"output_answer\": \"이자소득, 배당소득 및 기타 법에서 정한 소득은 원천징수 대상입니다.\"},\n",
    "        {\"output_answer\": \"공동 소유 자산의 양도소득은 각 거주자 소유 지분에 따라 과세됩니다.\"},\n",
    "        {\"output_answer\": \"이자 소득의 출처는 정부 및 지방자치단체가 발행한 채권, 법인이 발행한 채권, 국내외 은행 예금 등입니다.\"},\n",
    "        {\"output_answer\": \"배당소득은 국내외 법인으로부터 받는 배당금 및 배분금, 기타 법에서 정한 소득을 포함합니다.\"}\n",
    "            ],\n",
    "    metadata= [\n",
    "        {\"contexts\": \"제1조(목적) 이 법은 개인의 소득에 대하여 소득의 성격과 납세자의 부담능력 등에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지함을 목적으로 한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “거주자”란 국내에 주소를 두거나 183일 이상의 거소를 둔 개인을 말한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “비거주자”란 거주자가 아닌 개인을 말한다.\"},\n",
    "        {\"contexts\": \"제1조의2(정의) “내국법인”이란 「법인세법」 제2조제1호에 따른 내국법인을 말한다.\"},\n",
    "        {\"contexts\": \"제2조(납세의무) 거주자 및 국내원천소득이 있는 비거주자는 소득세를 납부할 의무가 있다.\"},\n",
    "        {\"contexts\": \"제3조(과세소득의 범위) 거주자는 법에서 규정한 모든 소득에 대해 과세되며, 비거주자는 국내원천소득에 대해서만 과세된다.\"},\n",
    "        {\"contexts\": \"제4조(소득의 구분) 소득은 종합소득, 퇴직소득, 양도소득으로 분류된다.\"},\n",
    "        {\"contexts\": \"제4조(소득의 구분) 종합소득은 이자소득, 배당소득, 사업소득, 근로소득, 연금소득 및 기타소득을 포함한다.\"},\n",
    "        {\"contexts\": \"제12조(비과세소득) 비과세 소득에는 공익신탁의 이익, 특정 사업소득 및 기타 법에서 정한 특정 소득이 포함된다.\"},\n",
    "        {\"contexts\": \"제5조(과세기간) 소득세의 과세기간은 매년 1월 1일부터 12월 31일까지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 거주자의 소득세 납세지는 주소지이며, 주소지가 없으면 거소지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 비거주자의 소득세 납세지는 국내사업장의 소재지이다. 국내사업장이 여러 곳인 경우 주된 사업장의 소재지이다.\"},\n",
    "        {\"contexts\": \"제6조(납세지) 납세지가 불분명한 경우에는 대통령령으로 정한다.\"},\n",
    "        {\"contexts\": \"제7조(원천징수 등의 경우의 납세지) 원천징수세액의 납세지는 원천징수자의 종류와 위치에 따라 결정된다.\"},\n",
    "        {\"contexts\": \"제8조(상속 등의 경우의 납세지) 납세자의 사망 시 상속인 또는 납세관리인의 주소지나 거소지가 납세지가 된다.\"},\n",
    "        {\"contexts\": \"제2조의3(신탁재산 귀속 소득에 대한 납세의무의 범위) 신탁 소득에 대한 납세의 범위는 신탁의 수익자가 해당 소득에 대해 납세의무를 진다.\"},\n",
    "        {\"contexts\": \"제14조(과세표준의 계산) 이자소득, 배당소득 및 기타 법에서 정한 소득은 원천징수 대상이다.\"},\n",
    "        {\"contexts\": \"제14조(과세표준의 계산) 공동 소유 자산의 양도소득은 각 거주자 소유 지분에 따라 과세된다.\"},\n",
    "        {\"contexts\": \"제16조(이자소득) 이자 소득의 출처는 정부 및 지방자치단체가 발행한 채권, 법인이 발행한 채권, 국내외 은행 예금 등이다.\"},\n",
    "        {\"contexts\": \"제17조(배당소득) 배당소득은 국내외 법인으로부터 받는 배당금 및 배분금, 기타 법에서 정한 소득을 포함한다.\"}\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"tax-markdown-index\"\n",
    "\n",
    "database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "retriever = database.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG bot\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "\n",
    "  def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "    self._retriever = retriever\n",
    "    self._client = wrap_openai(openai.Client())\n",
    "    self._model = model\n",
    "\n",
    "  @traceable\n",
    "  def retrieve_docs(self, question):\n",
    "    return self._retriever.invoke(question)\n",
    "  \n",
    "  @traceable\n",
    "  def invoke_llm(self, question, docs):\n",
    "    response = self._client.chat.completions.create(\n",
    "      model=self._model,\n",
    "      messages=[\n",
    "              {\n",
    "                  \"role\": \"system\",\n",
    "                  \"content\": \"당신은 한국의 소득세 전문가입니다.\"\n",
    "                  \"아래 소득세법을 참고해서 사용자의 질문에 답변해주세요.\\n\\n\"\n",
    "                  f\"## 소득세법\\n\\n{docs}\",\n",
    "              },\n",
    "              {\"role\": \"user\", \"content\": question},\n",
    "          ],\n",
    "    )\n",
    "\n",
    "    # Evaluators을 활용해서 'answer'와 'context'를 평가할 예정\n",
    "    return {\n",
    "      \"answer\": response.choices[0].message.content,\n",
    "      \"contexts\": [str(doc) for doc in docs],\n",
    "    }\n",
    "  \n",
    "  @traceable\n",
    "  def get_answer(self, question : str):\n",
    "    docs = self.retrieve_docs(question)\n",
    "    return self.invoke_llm(question, docs)\n",
    "  \n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"답변만 평가할 때 사용\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Context를 활용해서 hallucination을 평가할 때 사용\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-hub (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-hub\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "# Grade prompt\n",
    "# 답변의 정확도를 측정하기위해 사용되는 프롬프트\n",
    "grade_prompt_answer_accuracy = prompt = client.pull_prompt(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    RAG 답변 성능을 측정하기 위한 evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # `example`이 데이터를 생성할 때 입력한 `Question-Answer` pair. `run`은 `RagBot`을 활용해서 생성한 LLM의 답변\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "# 답변의 정확도를 측정하기위해 사용되는 프롬프트\n",
    "grade_prompt_answer_helpfulness = prompt = client.pull_prompt(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    답변이 사용자의 질문에 얼마나 도움되는지 판단하는 Evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터셋의 답변과 비교하지 않고, 데이터셋의 질문에 대한 LLM의 답변의 가치를 평가함\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "# hallucination 판단을 위한 프롬프트\n",
    "grade_prompt_hallucinations = prompt = client.pull_prompt(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    hallucination 판단을 위한 Evaluator\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터셋에 있는 질문과, LLM이 답변을 생성할 때 사용한 context를 활용\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # LLM의 답변\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM Judge로 사용될 LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # LLM 응답을 위한 LCEL 활용\n",
    "    # 3.6 `dictionary_chain`의 `prompt | llm | StrOutputParser()`` 의 구조와 유사함\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Evaluator 실행\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-v-reference-hallucination-1ca36dd8' at:\n",
      "https://smith.langchain.com/o/24233cbc-c0d7-40d1-9ed6-9e885d37dbd7/datasets/43c009e8-7892-4004-a351-7ab92f59d873/compare?selectedSessions=1907c960-18fb-404b-abf9-180a9c49be57\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [03:03, 14.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\_base_client.py:1027\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangsmith\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m      3\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mincome_tax_dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m experiment_results = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_rag_answer_with_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 어떤 함수를 활용해서 LLM 답변을 확인할지 지정, hallucination 판단 여부에 따라 `with_context` 사용\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Evaluation에 사용될 dataset의 이름\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43manswer_hallucination_evaluator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 실행할 Evaluator의 종류\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag-answer-v-reference-hallucination\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# experiment 이름 접두사\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mincome tax v1, gpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:423\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m     _warn_once(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mupload_results\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is in beta.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning evaluation over target system \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEVALUATOR_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:1101\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling)\u001b[39m\n\u001b[32m   1099\u001b[39m     manager = manager.with_summary_evaluators(summary_evaluators)\n\u001b[32m   1100\u001b[39m \u001b[38;5;66;03m# Start consuming the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m results = \u001b[43mExperimentResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:580\u001b[39m, in \u001b[36mExperimentResults.__init__\u001b[39m\u001b[34m(self, experiment_manager, blocking)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28mself\u001b[39m._thread = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:605\u001b[39m, in \u001b[36mExperimentResults._process_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    603\u001b[39m tqdm = _load_tqdm()\n\u001b[32m    604\u001b[39m results = \u001b[38;5;28mself\u001b[39m._manager.get_results()\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1175\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:1521\u001b[39m, in \u001b[36m_ExperimentManager.get_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterable[ExperimentResultRow]:\n\u001b[32m   1520\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the traces, evaluation results, and associated examples.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_results\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mExperimentResultRow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:1501\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# Split the generator into three so the manager\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# can consume each value individually.\u001b[39;00m\n\u001b[32m   1498\u001b[39m r1, r2, r3 = itertools.tee(experiment_results, \u001b[32m3\u001b[39m)\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copy(\n\u001b[32m   1500\u001b[39m     (result[\u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r1),\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     runs=\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1502\u001b[39m     evaluation_results=(result[\u001b[33m\"\u001b[39m\u001b[33mevaluation_results\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r3),\n\u001b[32m   1503\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:1691\u001b[39m, in \u001b[36m_ExperimentManager._score\u001b[39m\u001b[34m(self, evaluators, max_concurrency)\u001b[39m\n\u001b[32m   1689\u001b[39m     context = copy_context()\n\u001b[32m   1690\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m current_results \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_results():\n\u001b[32m-> \u001b[39m\u001b[32m1691\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m context.run(\n\u001b[32m   1692\u001b[39m             \u001b[38;5;28mself\u001b[39m._run_evaluators,\n\u001b[32m   1693\u001b[39m             evaluators,\n\u001b[32m   1694\u001b[39m             current_results,\n\u001b[32m   1695\u001b[39m             executor,\n\u001b[32m   1696\u001b[39m         )\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1698\u001b[39m     futures = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py:1619\u001b[39m, in \u001b[36m_ExperimentManager._run_evaluators\u001b[39m\u001b[34m(self, evaluators, current_results, executor)\u001b[39m\n\u001b[32m   1617\u001b[39m evaluator_run_id = uuid.uuid4()\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     evaluator_response = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     eval_results[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m].extend(\n\u001b[32m   1626\u001b[39m         \u001b[38;5;28mself\u001b[39m.client._select_eval_results(evaluator_response)\n\u001b[32m   1627\u001b[39m     )\n\u001b[32m   1628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._upload_results:\n\u001b[32m   1629\u001b[39m         \u001b[38;5;66;03m# TODO: This is a hack\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py:351\u001b[39m, in \u001b[36mDynamicRunEvaluator.evaluate_run\u001b[39m\u001b[34m(self, run, example, evaluator_run_id)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(run, \u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    350\u001b[39m     metadata[\u001b[33m\"\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(run.session_id)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlangsmith_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator_run_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_result(result, evaluator_run_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py:693\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    689\u001b[39m         function_result = run_container[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m].run(\n\u001b[32m    690\u001b[39m             run_with_otel_context\n\u001b[32m    691\u001b[39m         )\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m         function_result = \u001b[43mrun_container\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    697\u001b[39m     _cleanup_traceback(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36manswer_hallucination_evaluator\u001b[39m\u001b[34m(run, example)\u001b[39m\n\u001b[32m     22\u001b[39m answer_grader = grade_prompt_hallucinations | llm\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Evaluator 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m score = \u001b[43manswer_grader\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                              \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstudent_answer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m score = score[\u001b[33m\"\u001b[39m\u001b[33mScore\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33manswer_hallucination\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: score}\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5534\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5532\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5533\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:382\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    375\u001b[39m     **kwargs: Any,\n\u001b[32m    376\u001b[39m ) -> AIMessage:\n\u001b[32m    377\u001b[39m     config = ensure_config(config)\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    379\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    380\u001b[39m         cast(\n\u001b[32m    381\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    392\u001b[39m         ).message,\n\u001b[32m    393\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1101\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m     **kwargs: Any,\n\u001b[32m   1099\u001b[39m ) -> LLMResult:\n\u001b[32m   1100\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:911\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    910\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m         )\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    919\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1205\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1203\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1205\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1209\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1267\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1264\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m     )\n\u001b[32m   1271\u001b[39m     response = raw_response.parse()\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:183\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    178\u001b[39m         response_format=response_format,\n\u001b[32m    179\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    180\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\_base_client.py:1033\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1032\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\jwy\\learn-ai-tools\\venv\\Lib\\site-packages\\openai\\_base_client.py:1073\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1070\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1071\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m time.sleep(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"income_tax_dataset\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context, # 어떤 함수를 활용해서 LLM 답변을 확인할지 지정, hallucination 판단 여부에 따라 `with_context` 사용\n",
    "    data=dataset_name, # Evaluation에 사용될 dataset의 이름\n",
    "    evaluators=[answer_hallucination_evaluator], # 실행할 Evaluator의 종류\n",
    "    experiment_prefix=\"rag-answer-v-reference-hallucination\", # experiment 이름 접두사\n",
    "    metadata={\"version\": \"income tax v1, gpt-4o\"}, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
