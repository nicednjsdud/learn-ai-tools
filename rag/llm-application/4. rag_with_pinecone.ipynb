{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 문서의 내용을 읽는다.\n",
    "2. 문서를 쪼갠다.\n",
    "    - 토큰수 초과로 답변을 생성하지 못할 수 있고\n",
    "    - 문서가 길면 (인풋이 길면) 답변 생성이 오래걸림\n",
    "  \n",
    "3. 임베딩 -> 벡터 데이터베이스에 저장\n",
    "4. 질문이 있을 때, 벡터 데이터베이스에 유사도 검색\n",
    "5. 유사도 검색으로 가져온 문서로 LLM에 질문과 같이 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet docx2txt langchain-community\n",
    "%pip install -qU langchain-text-splitters\n",
    "%pip install -U langchain langchain-core langchainhub\n",
    "%pip install -U pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## chunk_size = 청크 하나를 구성하는 최대 토큰 수\n",
    "## chunk_overlap = 청크들이 겹치는 토큰 수\n",
    "## 예를 들어, chunk_size=1500, chunk_overlap=200이면\n",
    "## 각 청크는 최대 1500 토큰까지 포함할 수 있으며,\n",
    "## 각 청크는 이전 청크와 200 토큰이 겹치게 됩니다.\n",
    "## 이 설정은 문서의 연속성을 유지하면서도\n",
    "## 청크 간의 연결성을 높이는 데 도움이 됩니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1500, \n",
    "  chunk_overlap=200\n",
    "  )\n",
    "\n",
    "loader = Docx2txtLoader(\"./tax.docx\")\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name = 'tax-index'\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "if index_name not in [i[\"name\"] for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# ✅ 5️⃣ from_documents() 호출 시 pinecone_client 제거\n",
    "database = PineconeVectorStore.from_documents(\n",
    "    document_list,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'\n",
    "retrieved_docs = database.similarity_search(query = query, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"[Identity]\n",
    "- 당신은 최고의 한국 소득세 전문가 입니다.\n",
    "- [Context]를 참고해서 사용자의 질문에 답변해주세요.\n",
    "\n",
    "[Context]\n",
    "{retrieved_docs}\n",
    "\n",
    "[User Question]\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = llm.invoke(prompt)\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(database.as_retriever(), combine_docs_chain)\n",
    "\n",
    "rag_chain.invoke({\"question\": query, \"context\": some_retrieved_context})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
