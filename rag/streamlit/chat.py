import streamlit as st
from langchain_pinecone import PineconeVectorStore
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.output_parsers import StrOutputParser
from langsmith import Client
from pinecone import Pinecone , ServerlessSpec
import os


load_dotenv()

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")

st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def get_ai_message(user_message):
    
    embedding = OpenAIEmbeddings(model="text-embedding-3-large")

    index_name = 'tax-markdown-index'

    # Pinecone Vector DB ì—°ê²°
    database = PineconeVectorStore.from_existing_index(
    embedding=embedding,
    index_name=index_name,
    )
    
    llm = ChatOpenAI(model="gpt-4o")

    prompt = ChatPromptTemplate.from_template("""
    [Identity]
    - ë‹¹ì‹ ì€ ìµœê³ ì˜ í•œêµ­ ì„¸ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    - [Context]ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    [Context]
    {context}

    [User Question]
    {input}
    """)

    # LLMê³¼ Prompt ì¤€ë¹„
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)

    # databaseëŠ” PineconeVectorStore ê°™ì€ Retrieverë¡œ ë³€í™˜ ê°€ëŠ¥í•´ì•¼ í•¨
    retriever = database.as_retriever(search_kwargs={"k":3})

    # Retrieval + Combine ì—°ê²°
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

    # ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ë³€ê²½í•˜ëŠ” ì²´ì¸
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    # prompt ìƒì„±
    prompt = ChatPromptTemplate.from_template(f"""

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
    ë§Œì•½ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ë¯¸ ëª…í™•í•˜ë‹¤ë©´, ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
                                            
    [ì‚¬ì „] : {dictionary}

    [ì‚¬ìš©ì ì§ˆë¬¸] : {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()

    tax_chain = {"input" : dictionary_chain} | rag_chain
    ai_message = tax_chain.invoke({"question": user_message})

    return ai_message["answer"]

if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
  with st.chat_message("user"):
      st.write(user_question)

  st.session_state.message_list.append({"role": "user", "content": user_question})
  with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
    ai_message = get_ai_message(user_question)
    with st.chat_message("ai"):
        st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ""})