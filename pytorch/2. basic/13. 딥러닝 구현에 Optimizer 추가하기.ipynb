{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동 미분 (torch.autograd)\n",
    "\n",
    "* Pytorch의 autograd는 신경망 훈련을 지원하는 자동 미분 기능\n",
    "* torch.autograd 동작 방법\n",
    "\n",
    "  * 텐서에 .requires_grad 속성을 True로 설정하면, 이후의 텐서 모든 연산들을 추적함\n",
    "  * 텐서 .backward()를 호출하면, 연산에 연결된 각 텐서들의 미분 값을 계산하여, 각 텐서 객체.grad에 저장\n",
    "\n",
    "    * .requires_grad_()는 연결된 Tensor로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듬\n",
    "\n",
    "### 신경망 동작 이해\n",
    "\n",
    "* 모델 및 데이터 생성\n",
    "* forward pass로 입력 데이터를 모델에 넣어서 예측값 계산\n",
    "* 예측값과 실제값의 차이를 loss function으로 계산\n",
    "* backward pass 로 각 모델 파라미터를 loss 값 기반 미분하여 저장\n",
    "* optimizer로 모델 파라미터의 최적값을 찾기 위해, 파라미터 값 업데이트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드로 이해하는 autograd\n",
    "\n",
    "* 텐서에 .requires_grad 속성을 True로 설정\n",
    "* .requires_grad 속성이 True로 설정되면, 텐서의 모든 연산 추적을 위해, 내부적으로 방향성 비순환 그래프(DAG: Directed Acyclic Graph)를 동적 구성\n",
    "\n",
    "  * 방향성 비순환 그래프(DAG)의 leave 노드는 입력 텐서이고, root 노드는 결과 텐서가 됨\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
