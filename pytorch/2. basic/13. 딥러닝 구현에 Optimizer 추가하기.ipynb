{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동 미분 (torch.autograd)\n",
    "\n",
    "* Pytorch의 autograd는 신경망 훈련을 지원하는 자동 미분 기능\n",
    "* torch.autograd 동작 방법\n",
    "\n",
    "  * 텐서에 .requires_grad 속성을 True로 설정하면, 이후의 텐서 모든 연산들을 추적함\n",
    "  * 텐서 .backward()를 호출하면, 연산에 연결된 각 텐서들의 미분 값을 계산하여, 각 텐서 객체.grad에 저장\n",
    "\n",
    "    * .requires_grad_()는 연결된 Tensor로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듬\n",
    "\n",
    "### 신경망 동작 이해\n",
    "\n",
    "* 모델 및 데이터 생성\n",
    "* forward pass로 입력 데이터를 모델에 넣어서 예측값 계산\n",
    "* 예측값과 실제값의 차이를 loss function으로 계산\n",
    "* backward pass 로 각 모델 파라미터를 loss 값 기반 미분하여 저장\n",
    "* optimizer로 모델 파라미터의 최적값을 찾기 위해, 파라미터 값 업데이트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드로 이해하는 autograd\n",
    "\n",
    "* 텐서에 .requires_grad 속성을 True로 설정\n",
    "* .requires_grad 속성이 True로 설정되면, 텐서의 모든 연산 추적을 위해, 내부적으로 방향성 비순환 그래프(DAG: Directed Acyclic Graph)를 동적 구성\n",
    "\n",
    "  * 방향성 비순환 그래프(DAG)의 leave 노드는 입력 텐서이고, root 노드는 결과 텐서가 됨\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad=True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드로 이해하는 autograd\n",
    "\n",
    "* 입력 차원이 4, 출력 차원이 3이고, Linear Layer 함수를 $f(x) = x * W + b$으로 정의\n",
    "* requires_grad = True 인 텐서와 연결되어 계산되는 텐서는 grad_fn 속성을 가지며, 관련 정보는 backward()시 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2964, 0.4378, 0.6806],\n",
      "        [0.6602, 0.9106, 0.4719],\n",
      "        [0.2707, 0.8291, 0.5090],\n",
      "        [0.5135, 0.4646, 0.5162]], requires_grad=True) tensor([0.2041, 0.9772, 0.9917], requires_grad=True) tensor([1.9449, 3.6192, 3.1694], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "z = torch.matmul(x, W) + b\n",
    "print(W, b, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수와 MSE\n",
    "\n",
    "* MSE(Mean Squared Error) : 평균 제곱근 오차\n",
    "\n",
    "  * 실제 값과 예측 값의 차이를 제곱한 값의 평균\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.9756, grad_fn=<MseLossBackward0>) tensor([[1.2966, 2.4128, 2.1129],\n",
      "        [1.2966, 2.4128, 2.1129],\n",
      "        [1.2966, 2.4128, 2.1129],\n",
      "        [1.2966, 2.4128, 2.1129]]) tensor([1.2966, 2.4128, 2.1129])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(loss, W.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종결과: 1 tensor(0.0955, grad_fn=<MseLossBackward0>) tensor([0.2007, 0.3734, 0.3270], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "learning_rate = 0.01\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold:\n",
    "  iteration_num += 1\n",
    "  W = W - learning_rate * W.grad\n",
    "  b = b - learning_rate * b.grad\n",
    "  print(iteration_num, loss, z , y)\n",
    "\n",
    "  # detach_() : 텐서를 기존 방향성 비순환 그래프 (DAG : Directed Acyclic Graph)로 부터 끊음\n",
    "  # .requires_grad_(True) : 연결된 Tensor로부터의 계산된 자동미분 값을, 다시 현 텐서부터 시작하도록 만듬\n",
    "  W.detach_().requires_grad_(True)\n",
    "  b.detach_().requires_grad_(True)\n",
    "\n",
    "  z = torch.matmul(x, W) + b\n",
    "  loss = F.mse_loss(z, y)\n",
    "  loss.backward()\n",
    "\n",
    "print(\"최종결과:\", iteration_num + 1, loss, z , y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
