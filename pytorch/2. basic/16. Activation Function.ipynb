{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 활성화 함수\n",
    "\n",
    "* `__init__()` 함수에서는 모델에서 사용될 모듈(nn.Linear 등)과 activation function (활성화 함수) 등을 정의함\n",
    "* forward() 함수에서 실행되어야 하는 연산에 활성화 함수도 적용하면 됨\n",
    "* 주요 활성화 함수\n",
    "\n",
    "  * 시그모이드 함수 : nn.Sigmoid()\n",
    "  * ReLU 함수 : nn.ReLU()\n",
    "  * Leaky ReLU 함수 : nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()  # 부모 클래스의 생성자 호출\n",
    "    # 명시적으로 인자명을 써주는 경우도 많이 쓰임\n",
    "    self.linear = nn.Linear(in_features=input_dim, out_features=output_dim)\n",
    "    # self.activation = nn.Sigmoid() # 시그모이드 활성화 함수 추가\n",
    "    self.activation = nn.ReLU()  # ReLU 활성화 함수로 변경\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.linear(x))  # 선형 변환 후 시그모이드 활성화 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4) # 입력 데이터\n",
    "y = torch.zeros(3) # 타겟 데이터\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  \n",
    "  y_pred = model(x)  # 모델에 입력 데이터 전달\n",
    "  loss = loss_function(y_pred, y)  # 손실 계산\n",
    "\n",
    "  optimizer.zero_grad()  # 기울기 초기화\n",
    "  loss.backward()  # 역전파 수행\n",
    "  optimizer.step()  # 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3981e-14, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3357, -0.0423, -0.0352,  0.2639],\n",
      "        [ 0.1099, -0.1510,  0.0899, -0.2836],\n",
      "        [ 0.3906,  0.1287, -0.4643, -0.0335]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1494, -0.1243, -0.2704], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 레이어 구현\n",
    "\n",
    "* input layer -> hidden layer -> output layer 순으로 순차적으로 작성해주면 됨\n",
    "\n",
    "  * 내부 행렬곱 조건만 유의해주면 됨\n",
    "\n",
    "* activation function 적용은 output layer에는 적용하지 않는 것이 일반적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()  # 부모 클래스의 생성자 호출\n",
    "    self.linear1 = nn.Linear(input_dim, 10)\n",
    "    self.linear2 = nn.Linear(10, 10)\n",
    "    self.linear3 = nn.Linear(10, 10)\n",
    "    self.linear4 = nn.Linear(10, output_dim)\n",
    "    self.activation = nn.ReLU(0.1)  # ReLU 활성화 함수 사용\n",
    "\n",
    "  def forward(self, x):\n",
    "    # |x| = (input_dim, output_dim)\n",
    "    hidden = self.activation(self.linear1(x)) # |hidden| = (input_dim, 5)\n",
    "    hidden = self.activation(self.linear2(hidden)) # |hidden| = (5, 5)\n",
    "    hidden = self.activation(self.linear3(hidden)) # |hidden| = (5, 5)\n",
    "    y = self.linear4(hidden) # 마지막 출력에는 activation 함수 사용하지 않는 것이 일반적\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4) # 입력 데이터\n",
    "y = torch.zeros(3) # 타겟 데이터\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  y_pred = model(x)  # 모델에 입력 데이터 전달\n",
    "  loss = loss_function(y_pred, y)  # 손실 계산\n",
    "\n",
    "  optimizer.zero_grad()  # 기울기 초기화\n",
    "  loss.backward()  # 역전파 수행\n",
    "  optimizer.step()  # 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6077e-12, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1204,  0.4659,  0.1615, -0.4481],\n",
      "        [-0.0839,  0.1950, -0.3970,  0.2374],\n",
      "        [-0.0385, -0.3775, -0.4988,  0.0946],\n",
      "        [-0.4180, -0.1393, -0.4911,  0.3324],\n",
      "        [-0.2099, -0.3720, -0.0397, -0.2565],\n",
      "        [-0.2442, -0.4150, -0.1952, -0.2070],\n",
      "        [ 0.1258, -0.1804,  0.0869, -0.3465],\n",
      "        [ 0.3508, -0.0824,  0.1861,  0.1149],\n",
      "        [-0.1007,  0.4691, -0.1621, -0.1505],\n",
      "        [-0.1919, -0.2876, -0.0688,  0.1972]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4891, -0.0945, -0.1008,  0.4157, -0.2431, -0.0073, -0.1456, -0.4737,\n",
      "        -0.1212, -0.4458], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1044, -0.2551,  0.1321, -0.0971, -0.2382,  0.2262, -0.0278, -0.0882,\n",
      "         -0.1404, -0.1707],\n",
      "        [-0.0635,  0.2739,  0.0013, -0.0202,  0.1146,  0.1113, -0.2956,  0.1518,\n",
      "         -0.0734,  0.2853],\n",
      "        [-0.0194, -0.1272, -0.0691, -0.0453,  0.1260, -0.2701,  0.1709, -0.0052,\n",
      "         -0.0076, -0.2243],\n",
      "        [ 0.0465,  0.1825,  0.0939,  0.0198, -0.2127,  0.2977, -0.2050, -0.2385,\n",
      "         -0.2914,  0.1462],\n",
      "        [ 0.3179,  0.1724, -0.1199, -0.3061, -0.2915,  0.2908,  0.2803, -0.0429,\n",
      "         -0.2471, -0.3028],\n",
      "        [-0.0658, -0.0405,  0.1527,  0.0204,  0.1766,  0.1269,  0.2152,  0.0972,\n",
      "          0.0291, -0.2392],\n",
      "        [ 0.1475,  0.2543, -0.0393, -0.1986,  0.1244,  0.2769,  0.2627, -0.2086,\n",
      "         -0.0346,  0.2362],\n",
      "        [ 0.3050,  0.0271,  0.0305, -0.2864,  0.0288, -0.2901, -0.2283,  0.1520,\n",
      "         -0.0357, -0.1400],\n",
      "        [-0.0574, -0.2184, -0.1447,  0.2113, -0.2782,  0.0831, -0.2382, -0.3143,\n",
      "          0.0749, -0.0952],\n",
      "        [-0.2683,  0.1334,  0.3014, -0.1839, -0.0009, -0.2693, -0.2201,  0.0609,\n",
      "         -0.1399, -0.0917]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2749,  0.0356, -0.0696,  0.0896, -0.0547, -0.2814, -0.2508,  0.1820,\n",
      "         0.0221,  0.1375], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2252,  0.2388,  0.2186,  0.2124,  0.1332,  0.2650, -0.0456, -0.2217,\n",
      "          0.0436, -0.1302],\n",
      "        [-0.0570, -0.2751, -0.2023,  0.0186,  0.1093,  0.1229,  0.1085,  0.2746,\n",
      "          0.2637,  0.0619],\n",
      "        [ 0.2053, -0.1350, -0.0020,  0.1439, -0.1224,  0.1773, -0.0983, -0.1092,\n",
      "         -0.2706, -0.0058],\n",
      "        [ 0.1875,  0.1604, -0.0358,  0.1478,  0.1739, -0.1069,  0.1025, -0.1552,\n",
      "         -0.2214,  0.0567],\n",
      "        [ 0.0759,  0.1542, -0.0514, -0.1920, -0.0388,  0.1382,  0.0391,  0.3162,\n",
      "          0.1405,  0.2968],\n",
      "        [ 0.1312,  0.1618,  0.3066,  0.2041, -0.0175,  0.0169,  0.1857,  0.1056,\n",
      "         -0.2780,  0.0568],\n",
      "        [-0.2107,  0.0610,  0.1530,  0.1815, -0.1547, -0.1579, -0.1558,  0.2270,\n",
      "         -0.0083, -0.2739],\n",
      "        [-0.1996,  0.1712, -0.1577, -0.3051, -0.2748,  0.1140,  0.0120, -0.2833,\n",
      "          0.1886, -0.2654],\n",
      "        [-0.1600,  0.3016, -0.0534,  0.0921, -0.0586, -0.0414, -0.3099, -0.1386,\n",
      "         -0.0917,  0.2521],\n",
      "        [ 0.1978,  0.1520, -0.1478, -0.3055, -0.1871,  0.2907, -0.2789, -0.0748,\n",
      "         -0.1711,  0.2599]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1825,  0.2707, -0.1107, -0.1908,  0.2722,  0.1329, -0.0386,  0.2023,\n",
      "         0.0308,  0.1840], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0957,  0.3124, -0.1803,  0.0519,  0.3023, -0.2047, -0.0839, -0.1498,\n",
      "          0.1766, -0.1708],\n",
      "        [-0.0682,  0.0233, -0.2962,  0.1246, -0.0657, -0.1042, -0.0599, -0.0227,\n",
      "          0.0047, -0.0423],\n",
      "        [ 0.0945, -0.1957,  0.1320,  0.0413, -0.2582, -0.2867, -0.2748,  0.1478,\n",
      "          0.0073,  0.0799]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1863,  0.0426,  0.2433], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "\n",
    "* nn.Sequential은 순서를 갖는 모듈의 컨테이너를 의미함\n",
    "* 순차적으로 연산되는 레이어만 있을 경우에는, nn.Sequential 을 통해 순서대로 각 레이어를 작성하면 그대로 실행됨\n",
    "\n",
    "  * 중간에 activation function이 적용된다면, activation function도 순서에 맞게 넣어주면 자동 계산 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(0)\n",
    "output_dim = y.size(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(10, output_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  y_pred = model(x)  # 모델에 입력 데이터 전달\n",
    "  loss = loss_function(y_pred, y)  # 손실 계산\n",
    "\n",
    "  optimizer.zero_grad()  # 기울기 초기화\n",
    "  loss.backward()  # 역전파 수행\n",
    "  optimizer.step()  # 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0070e-13, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.4381,  0.3992,  0.2607, -0.2762],\n",
      "        [-0.1656, -0.2880,  0.0890, -0.2153],\n",
      "        [ 0.4674,  0.3049, -0.1792, -0.0701],\n",
      "        [ 0.1322,  0.0445,  0.3819,  0.2812],\n",
      "        [ 0.1160,  0.3883,  0.1645, -0.4500],\n",
      "        [-0.4363,  0.0233,  0.2574,  0.3525],\n",
      "        [ 0.4328, -0.4439,  0.1300, -0.2645],\n",
      "        [-0.4831, -0.4115,  0.2795, -0.4369],\n",
      "        [-0.1175, -0.4448, -0.3968, -0.3317],\n",
      "        [-0.0493,  0.2065,  0.3251,  0.4387]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2167,  0.4244, -0.1690,  0.3955,  0.2108,  0.3322,  0.2178, -0.3951,\n",
      "        -0.1961, -0.2803], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0328, -0.1617, -0.0312,  0.2393,  0.0604,  0.2848,  0.1242,  0.1320,\n",
      "          0.2879,  0.0743],\n",
      "        [ 0.2898,  0.0837,  0.2301, -0.2395, -0.0403,  0.1466,  0.0244,  0.1484,\n",
      "         -0.2794, -0.3033],\n",
      "        [ 0.0583,  0.2014, -0.0573, -0.1114, -0.3068, -0.0056, -0.1658, -0.0319,\n",
      "         -0.0954,  0.0389],\n",
      "        [ 0.2873,  0.2352, -0.3100, -0.0048,  0.0718,  0.2530,  0.0549, -0.1749,\n",
      "         -0.3144,  0.2460],\n",
      "        [-0.2753,  0.1860,  0.0734,  0.0182, -0.2726,  0.2426, -0.2238, -0.1869,\n",
      "          0.2309, -0.2959],\n",
      "        [-0.0024, -0.0909, -0.1473, -0.1347,  0.3010,  0.1612,  0.2957,  0.1759,\n",
      "         -0.2355, -0.2194],\n",
      "        [ 0.0258,  0.0403, -0.1676, -0.2250, -0.2016, -0.0899, -0.1461,  0.2347,\n",
      "          0.0670,  0.0059],\n",
      "        [-0.2258, -0.2770,  0.0394,  0.2108, -0.1297,  0.2449,  0.2316, -0.0189,\n",
      "         -0.1321,  0.1589],\n",
      "        [-0.0865,  0.2982,  0.0344,  0.1817, -0.3158,  0.3110, -0.1744,  0.2766,\n",
      "          0.1034,  0.1573],\n",
      "        [-0.1349, -0.0662,  0.3147,  0.1536, -0.1319, -0.0767, -0.2147, -0.1532,\n",
      "         -0.0934,  0.0064]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1852,  0.0672, -0.0462,  0.0070,  0.0039, -0.0887,  0.2944, -0.0860,\n",
      "         0.1943, -0.2359], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0319, -0.0556, -0.1771,  0.3083,  0.1737,  0.2146, -0.0760,  0.0531,\n",
      "          0.3158, -0.2585],\n",
      "        [-0.3029,  0.3157, -0.0294,  0.1690,  0.2833, -0.1670,  0.1601,  0.0807,\n",
      "          0.2480,  0.2570],\n",
      "        [ 0.2848,  0.1581,  0.2149, -0.2628, -0.1526,  0.0442,  0.2101, -0.0724,\n",
      "          0.0061, -0.1901],\n",
      "        [-0.2601,  0.1947,  0.2761, -0.1042, -0.3018,  0.0016,  0.2452, -0.1823,\n",
      "         -0.1796,  0.0165],\n",
      "        [ 0.0594, -0.0958, -0.2613, -0.2194, -0.1792,  0.0065, -0.0490, -0.0382,\n",
      "         -0.1172, -0.0248],\n",
      "        [ 0.0147, -0.1680,  0.2035, -0.0800, -0.3123,  0.2892,  0.2548, -0.2564,\n",
      "         -0.0872, -0.2306],\n",
      "        [-0.1230, -0.1000, -0.2075,  0.1007, -0.1836,  0.0772, -0.0213,  0.2724,\n",
      "          0.1872,  0.1735],\n",
      "        [ 0.1952, -0.3089, -0.0345,  0.1235, -0.3132,  0.0292,  0.0392,  0.2753,\n",
      "         -0.0865,  0.3099],\n",
      "        [-0.0944, -0.1423,  0.0479, -0.0384,  0.0403, -0.0125, -0.0014,  0.0976,\n",
      "         -0.3139, -0.0748],\n",
      "        [ 0.2250, -0.1114, -0.0503, -0.2722, -0.0304,  0.0097,  0.0411,  0.0210,\n",
      "         -0.1589, -0.1334]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1089, -0.0624, -0.0359,  0.0882,  0.2144, -0.0767, -0.1019,  0.0729,\n",
      "        -0.1088,  0.0940], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0030,  0.2139,  0.0990, -0.1097, -0.2244,  0.2359,  0.0271, -0.3254,\n",
      "         -0.2127,  0.0943],\n",
      "        [ 0.1690, -0.1699,  0.1861, -0.2887,  0.2997, -0.1684, -0.2325, -0.2756,\n",
      "         -0.2428, -0.1339],\n",
      "        [-0.2424,  0.1947,  0.0533, -0.2966, -0.2333, -0.2424, -0.2582,  0.0778,\n",
      "         -0.0562,  0.2390]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0873, 0.0259, 0.0469], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
