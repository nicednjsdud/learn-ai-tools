{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer와 경사 하강법\n",
    "\n",
    "* 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정으로 Optimizer는 최적화 알고리즘을 의미함.\n",
    "* 대표적인 최적화 알고리즘이 확률적 경사하강법 (SGD : Stochastic Gradient Descent)\n",
    "* PyTorch에는 모델과 데이터 타입에 따라, 보다 좋은 성능을 제공하는 ADRAM이나 RMSProp과 같은 다양한 옵티마이저가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 경사 하강법 Optimizer 사용법\n",
    "\n",
    "* 초기화 (모델 파라미터 리스트와 learning rate(학습률)을 넣어줌)\n",
    "\n",
    "  * optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "* 학습 단계 (반복 수행하며 최적화하는 단계)\n",
    "\n",
    "  * optimizer.zero_grad()를 호출하여 모델 파라미터의 미분값(변화도)를 0으로 초기화 해줌\n",
    "  * loss.backward()를 호출하여 backward pass를 계산하고, 연산에 연결된 각 텐서들의 미분 값을 각 텐서 객체.grad에 저장\n",
    "  * optimizer.step()을 호출하여 역전파 단계에서 계산된 미분값(변화도)를 기반으로 모델 파라미터 값 업데이트\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer.zero_grad() 이해\n",
    "\n",
    "* 모델 파라미터의 미분값(변화도) 을 초기화해주지 않으면, 이어지는 모델 파라미터 연산시, 해당 연산에 대한 미분값(변화도)에, 기존 미분값(변화도)를 더해줌\n",
    "* 따라서, 해당 연산에 대한 미분값(변화도)가 필요한 경우, optimizer.zero_grad()를 통해 기존 미분값(변화도)을 0으로 초기화 해줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)  # Output: tensor(2.)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward()\n",
    "print(w.grad)  # Output: tensor(4.)\n",
    "\n",
    "z = 2 * w\n",
    "z.backward() \n",
    "print(w.grad)  # Output: tensor(2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 경사 하강법 Optimizer 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "W = torch.rand(4, 3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD([W, b], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 tensor([[0.7349, 0.5981, 0.3444],\n",
      "        [0.3766, 0.3881, 0.2666],\n",
      "        [0.7156, 0.8437, 0.2167],\n",
      "        [0.7311, 0.4108, 0.5383]], requires_grad=True) tensor([0.2145, 0.6679, 0.0928], requires_grad=True) tensor(6.5196, grad_fn=<MseLossBackward0>)\n",
      "100 300 tensor([[ 0.1990,  0.0359,  0.0625],\n",
      "        [-0.1593, -0.1740, -0.0153],\n",
      "        [ 0.1798,  0.2816, -0.0652],\n",
      "        [ 0.1953, -0.1513,  0.2564]], requires_grad=True) tensor([-0.3213,  0.1058, -0.1891], requires_grad=True) tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "200 300 tensor([[ 0.1810,  0.0170,  0.0530],\n",
      "        [-0.1773, -0.1930, -0.0248],\n",
      "        [ 0.1617,  0.2627, -0.0747],\n",
      "        [ 0.1772, -0.1703,  0.2469]], requires_grad=True) tensor([-0.3394,  0.0868, -0.1986], requires_grad=True) tensor(8.4124e-06, grad_fn=<MseLossBackward0>)\n",
      "300 300 tensor([[ 0.1804,  0.0164,  0.0526],\n",
      "        [-0.1779, -0.1936, -0.0251],\n",
      "        [ 0.1611,  0.2620, -0.0750],\n",
      "        [ 0.1766, -0.1709,  0.2466]], requires_grad=True) tensor([-0.3400,  0.0862, -0.1990], requires_grad=True) tensor(9.5607e-09, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 300 # 원하는 만큼 경사 하강법 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "  z = torch.matmul(x, W) + b  # 예측값 계산\n",
    "  loss = F.mse_loss(z, y)  # 실제값과의 오차 계산\n",
    "\n",
    "  optimizer.zero_grad()  # 기울기 초기화\n",
    "  loss.backward()  # 기울기 계산\n",
    "  optimizer.step()  # 매개변수 갱신\n",
    "  \n",
    "\n",
    "  # 100번마다 로그 출력\n",
    "  if epoch % 100 == 0:\n",
    "    print(epoch, nb_epochs, W, b, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD 경사 하강법 Optimizer 적용 (PyTorch 신경망 모델 클래스 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel(4, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
