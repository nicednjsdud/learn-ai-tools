{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치로 Linear Layer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 모델 용어 정리\n",
    "\n",
    "* 계층(layer) : 모듈 또는 모듈을 구성하는 한 개의 계층을 의미함 (예: 선형 계층, Linear Layer)\n",
    "* 모듈(module) : 한 개 이상의 계층이 모여 구성된 것. 모듈이 모여서 새로운 모듈 구성 가능\n",
    "* 모델(model) : 최종적인 네트워크, 한 개의 모듈이 모델이 될 수도 있고, 여러 개의 모듈이 하나의 모델이 될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn 과 nn.Module\n",
    "\n",
    "* torch.nn 네임스페이스는 신경망을 구성하는데 필요한 모든 구성 요소 제공\n",
    "* 모든 PyTorch 모듈은 nn.Module의 하위 클래스(subclass) 임\n",
    "* 모든 PyTorch 신경망 모델은 nn.Module을 상속 받은 하위 클래스로 정의함\n",
    "\n",
    "  * `__init__` 에서 신경망 계층 초기화 필요\n",
    "  * forward() 메서드에서 입력 데이터에 대한 연산 정의 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw-level Linear Layer 구현\n",
    "\n",
    "* 벡터 X 행렬은 벡터의 차원 수를 (a), 행렬의 열의 수 (b, c)일 때, a와 b가 같아야하고,\n",
    "* 이때 결과 shape 는 (a) x (a, c) = (c)가됨\n",
    "* (4) x (4, 3) = (3)이 되고, (3) + (3) = (3)이 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4) # 입력\n",
    "W = torch.FloatTensor(4, 3) # 가중치\n",
    "b = torch.FloatTensor(3) # 편향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Layer 함수를 정의한다면\n",
    "def linearFunction(x, W, b):\n",
    "  y = torch.matmul(x, W) + b\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W torch.Size([4, 3]) tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "b torch.Size([3]) tensor([0., 0., 0.])\n",
      "\n",
      "\n",
      "y torch.Size([3]) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"W\" , W.shape, W)\n",
    "print(\"b\" , b.shape, b)\n",
    "y = linearFunction(x, W, b)\n",
    "print(\"\\n\")\n",
    "print(\"y\" , y.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module 기반, Linear Layer 구현\n",
    "\n",
    "* 신경망 모델 클래스를 만들고, nn.Module을 상속받음\n",
    "* `__init__`에서 신경망 계층 초기화 선언\n",
    "* forward() 메서드에서 입력 데이터에 대한 연산 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    # super() 함수는 super(subclass 이름, subclass 객체).__init__() 와 같이 써야 하지만,\n",
    "    # 하부 클래스 선언 내부에서 super() 호출 시는 super().__init__()와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "    super().__init__()\n",
    "    # __init__() 에서 신경망 계층 초기화\n",
    "    self.W = torch.FloatTensor(4, 3)\n",
    "    self.b = torch.FloatTensor(3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # |x| = (input_dim)\n",
    "    # |y| = (input_dim) * (input_dim, output_dim) + (output_dim) = (output_dim)\n",
    "    y = torch.matmul(x, self.W) + self.b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.5095e-09, 1.5638e-42, 2.3694e-38]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "myLinear = NeuralNetwork()\n",
    "# forward에 넣을 인자값으로 호출하면, 내부적으로 forward() 함수가 자동 호출함 (정석적 방법)\n",
    "# 내부 처리중, forward() 전처리/ 후처리도 수행하므로, forward()를 직접 호출하면, 전처리/후처리를 수행하지 않게 될 수 있음\n",
    "y = myLinear(x)\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선된 Linear\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    # super() 함수는 super(subclass 이름, subclass 객체).__init__() 와 같이 써야 하지만,\n",
    "    # 하부 클래스 선언 내부에서 super() 호출 시는 super().__init__()와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "    super().__init__()\n",
    "    # __init__() 에서 신경망 계층 초기화\n",
    "    self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "    self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # |x| = (input_dim)\n",
    "    # |y| = (input_dim) * (input_dim, output_dim) + (output_dim) = (output_dim)\n",
    "    y = torch.matmul(x, self.W) + self.b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1876, 0.0000]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "myLinear = NeuralNetwork(4, 3)\n",
    "y = myLinear(x)\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in myLinear.parameters():\n",
    "  print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module 기반, nn.Parameter 등록하기\n",
    "\n",
    "* 학습 대상이 되는 텐서는 해당 모듈에 연결된 Parameter로 등록해야 함\n",
    "\n",
    "  * 이를 통해 특정 모듈에서 학습 처리시 필요한 작업을 알아서 해주도록 구성되어 있음\n",
    "  * 모듈 내에서 학습 대상이 되는 텐서들은 `__init__`에서 nn.Parameter()으로 등록해줘야 함\n",
    "\n",
    "    * nn.Parameter(텐서, requires_grad = True)\n",
    "\n",
    "      * requires_grad 는 디폴트는 True이며, 파라미터가 gradient(점진적으로 최적값을 찾아가는 방식) 방식으로 계산되는 케이스를 의미함\n",
    "      * 내부적으로 gradient 방식에서 필요한 미분 연산을 위해, 파이토치는 동적으로 그래프를 구성하므로, 이런 구성도 필요한 파라미터임을 의미함\n",
    "\n",
    "* 기본 신경망 모델 코드 작성 방법\n",
    "\n",
    "  1. 신경망 모델 클래스를 만들고, nn.Module을 상속받음\n",
    "  2. `__init__`에서 신경망 계층 초기화 선언 (모듈 내에서 학습 대상이 되는 텐서들은 nn.Parameter()으로 등록)\n",
    "  3. forward() 메서드에서 입력 데이터에 대한 연산 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    # super() 함수는 super(subclass 이름, subclass 객체).__init__() 와 같이 써야 하지만,\n",
    "    # 하부 클래스 선언 내부에서 super() 호출 시는 super().__init__()와 같이 쓰면, 자동으로 두 인자가 넣어져서 호출됨\n",
    "    super().__init__()\n",
    "    # __init__() 에서 신경망 계층 초기화\n",
    "    self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "    self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # |x| = (input_dim)\n",
    "    # |y| = (input_dim) * (input_dim, output_dim) + (output_dim) = (output_dim)\n",
    "    y = torch.matmul(x, self.W) + self.b\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
