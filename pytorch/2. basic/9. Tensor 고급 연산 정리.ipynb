{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 고급 연산 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch와 텐서간의 곱셈\n",
    "\n",
    "* torch.matmul() 메서드가 다양한 텐서 곱셈을 지원하므로, 상세히 이해할 필요가 있음\n",
    "* 이외에 torch.mm()은 2D 텐서, torch.bmm()은 3D 텐서 간의 연산만 지원하며, matmul()과의 차이점도 알아둘 필요가 있음\n",
    "\n",
    "  * matmul()은 텐서의 shape 등에 따라, 다양한 계산이 가능하고, broadcasting도 지원하므로, 자칫 예상치 못한 연산이 될 수도 있음\n",
    "  * 디버깅을 위해, 기대한 케이스에 대해서만 명확한 계산을 하는 것이 필요하다면, torch.mm(), torch.bmm() 사용을 고려할 필요도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 1D 텐서(벡터)의 곱셈\n",
    "\n",
    "* 1D 텐서(벡터) 끼리의 곱셈은 벡터의 내적값, 즉 스칼라(scala) 값을 리턴함\n",
    "* 두 벡터는 동일 차원이어야 함\n",
    "* 벡터의 내적은 선형대수에서 나오는 수식으로, dot product 라고도 불리움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2]) 1 torch.Size([3])\n",
      "tensor([3, 3, 3]) 1 torch.Size([3])\n",
      "tensor(18)\n"
     ]
    }
   ],
   "source": [
    "A = torch.full((3, ),2) # vector 생성\n",
    "B = torch.full((3, ),3) # vector 생성\n",
    "\n",
    "print(A, A.dim(), A.shape)\n",
    "print(B, B.dim(), B.shape)\n",
    "\n",
    "result = torch.matmul(A, B) # 내적 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D 텐서(행렬) X 2D 텐서(행렵) 곱\n",
    "\n",
    "* Matrix Multiplication, Inner Product, 또는 Dot Product 라고 부름\n",
    "* 앞 행렬의 열의 갯수와 뒷 행렬의 행의 갯수가 같아야 행렬간 곱셈이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2],\n",
      "        [2, 2],\n",
      "        [2, 2]]) 2 torch.Size([3, 2])\n",
      "tensor([[3, 3, 3],\n",
      "        [3, 3, 3]]) 2 torch.Size([2, 3])\n",
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.full((3, 2),2) # matrix 생성\n",
    "B = torch.full((2, 3),3) # matrix 생성\n",
    "print(A, A.dim(), A.shape)\n",
    "print(B, B.dim(), B.shape)\n",
    "result = torch.matmul(A, B) # 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 2D 텐서(행렬)의 곱\n",
    "\n",
    "* 벡터(vector) x 행렬(matrix) 와 행렬(matrix) x 벡터(vector)는 계산식이 다름\n",
    "* 벡터는 기본적으로 열 벡터로 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 18])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((3, ), 2) # vector 생성\n",
    "A = torch.full((3, 2), 3) # matrix 생성\n",
    "\n",
    "result = torch.matmul(x, A) # vector와 matrix의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 3D 이상 텐서의 곱\n",
    "\n",
    "* 1D 텐서(벡터)를 (a)라 하고,\n",
    "* 3D 이상 텐서를 (b,c,d)라 하면\n",
    "  * 3D 이상 텐서는 batched matrix로 간주하여, b x (c, d)가 됨\n",
    "* 즉, b x((a) x (c,d))가 되므로, 1D 텐서 X 2D 텐서와 마찬가지로, a와 c로 동일해야 함\n",
    "* 즉, (a) x (b, a, d)가 되고, 결과 shape는 동일한 a는 삭제되고, (b,d)가 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((3,), 2 ) # vector 생성\n",
    "A = torch.full((4, 3, 2), 3) # 3차원 tensor 생성\n",
    "result = torch.matmul(x, A) # vector와 3차원 tensor의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D 텐서(행렬) X 1D 텐서(벡터)\n",
    "\n",
    "* 행렬(matrix) x 벡터(vector) 는 행렬의 열의 수 (a, b)일 때, b와, 벡터의 차원 수(c)라고 할대, c가 같아야 하므로, \n",
    "* (a,b) x (b)가 되며, 결과 shape는 b가 삭제된 (a)가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((3,2), 3)\n",
    "\n",
    "result = torch.matmul(A, x) # matrix와 vector의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D 이상 텐서(행렬) X 1D 텐서(벡터)의 곱\n",
    "\n",
    "* 3D이상 텐서를 (a, b, c)라고 하고,\n",
    "  * 3D 이상 텐서는 batched matrix로 간주하여, a x (b, c)가 됨\n",
    "* 1D 텐서(벡터)를 (d)라 하면,\n",
    "* 즉, a x((b, c) x (d))가 되므로, 2D 텐서 X 1D 텐서 와 마찬가지로, c와 d는 동일해야 함\n",
    "* 즉, (a,b,c) x (c) 가 되고, 결과 shape는 동일한 c는 삭제되고, (a,b)가  됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((4, 3, 2), 3) # 3차원 tensor 생성\n",
    "result = torch.matmul(A, x) # 3차원 tensor와 vector의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n - Dㅌ텐서 (n > 2)간의 곱셈\n",
    "\n",
    "* 기본적으로 첫번째 인자와 두번째 인자의 마지막 두 axis 차원 이외의 동일함을 가정하고, Batched Matrix Multiplication 방식으로 계산됨\n",
    "  \n",
    "  * Batched Matrix Multiplication 방식이해 \n",
    "\n",
    "    * (b, m, n)은 (m, n) 행렬이 b개 있는 것이라고 볼 수 있음\n",
    "    * (b, m, n) x (b, n, k)는 b 개의 (m, n)을 b개의 (n, k)와 곱하는 것이라고 볼수 있음\n",
    "    * 따라서, tensor 간의 곱에서는 b자리는 동일해야 하며, (m, n), (n, k)는 행렬 곱셈과 동일한 제약사항을 가짐\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
