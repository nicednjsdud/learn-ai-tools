{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 고급 연산 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch와 텐서간의 곱셈\n",
    "\n",
    "* torch.matmul() 메서드가 다양한 텐서 곱셈을 지원하므로, 상세히 이해할 필요가 있음\n",
    "* 이외에 torch.mm()은 2D 텐서, torch.bmm()은 3D 텐서 간의 연산만 지원하며, matmul()과의 차이점도 알아둘 필요가 있음\n",
    "\n",
    "  * matmul()은 텐서의 shape 등에 따라, 다양한 계산이 가능하고, broadcasting도 지원하므로, 자칫 예상치 못한 연산이 될 수도 있음\n",
    "  * 디버깅을 위해, 기대한 케이스에 대해서만 명확한 계산을 하는 것이 필요하다면, torch.mm(), torch.bmm() 사용을 고려할 필요도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 1D 텐서(벡터)의 곱셈\n",
    "\n",
    "* 1D 텐서(벡터) 끼리의 곱셈은 벡터의 내적값, 즉 스칼라(scala) 값을 리턴함\n",
    "* 두 벡터는 동일 차원이어야 함\n",
    "* 벡터의 내적은 선형대수에서 나오는 수식으로, dot product 라고도 불리움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2]) 1 torch.Size([3])\n",
      "tensor([3, 3, 3]) 1 torch.Size([3])\n",
      "tensor(18)\n"
     ]
    }
   ],
   "source": [
    "A = torch.full((3, ),2) # vector 생성\n",
    "B = torch.full((3, ),3) # vector 생성\n",
    "\n",
    "print(A, A.dim(), A.shape)\n",
    "print(B, B.dim(), B.shape)\n",
    "\n",
    "result = torch.matmul(A, B) # 내적 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D 텐서(행렬) X 2D 텐서(행렵) 곱\n",
    "\n",
    "* Matrix Multiplication, Inner Product, 또는 Dot Product 라고 부름\n",
    "* 앞 행렬의 열의 갯수와 뒷 행렬의 행의 갯수가 같아야 행렬간 곱셈이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2],\n",
      "        [2, 2],\n",
      "        [2, 2]]) 2 torch.Size([3, 2])\n",
      "tensor([[3, 3, 3],\n",
      "        [3, 3, 3]]) 2 torch.Size([2, 3])\n",
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.full((3, 2),2) # matrix 생성\n",
    "B = torch.full((2, 3),3) # matrix 생성\n",
    "print(A, A.dim(), A.shape)\n",
    "print(B, B.dim(), B.shape)\n",
    "result = torch.matmul(A, B) # 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 2D 텐서(행렬)의 곱\n",
    "\n",
    "* 벡터(vector) x 행렬(matrix) 와 행렬(matrix) x 벡터(vector)는 계산식이 다름\n",
    "* 벡터는 기본적으로 열 벡터로 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 18])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((3, ), 2) # vector 생성\n",
    "A = torch.full((3, 2), 3) # matrix 생성\n",
    "\n",
    "result = torch.matmul(x, A) # vector와 matrix의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D 텐서(벡터) X 3D 이상 텐서의 곱\n",
    "\n",
    "* 1D 텐서(벡터)를 (a)라 하고,\n",
    "* 3D 이상 텐서를 (b,c,d)라 하면\n",
    "  * 3D 이상 텐서는 batched matrix로 간주하여, b x (c, d)가 됨\n",
    "* 즉, b x((a) x (c,d))가 되므로, 1D 텐서 X 2D 텐서와 마찬가지로, a와 c로 동일해야 함\n",
    "* 즉, (a) x (b, a, d)가 되고, 결과 shape는 동일한 a는 삭제되고, (b,d)가 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((3,), 2 ) # vector 생성\n",
    "A = torch.full((4, 3, 2), 3) # 3차원 tensor 생성\n",
    "result = torch.matmul(x, A) # vector와 3차원 tensor의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D 텐서(행렬) X 1D 텐서(벡터)\n",
    "\n",
    "* 행렬(matrix) x 벡터(vector) 는 행렬의 열의 수 (a, b)일 때, b와, 벡터의 차원 수(c)라고 할대, c가 같아야 하므로, \n",
    "* (a,b) x (b)가 되며, 결과 shape는 b가 삭제된 (a)가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((3,2), 3)\n",
    "\n",
    "result = torch.matmul(A, x) # matrix와 vector의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D 이상 텐서(행렬) X 1D 텐서(벡터)의 곱\n",
    "\n",
    "* 3D이상 텐서를 (a, b, c)라고 하고,\n",
    "  * 3D 이상 텐서는 batched matrix로 간주하여, a x (b, c)가 됨\n",
    "* 1D 텐서(벡터)를 (d)라 하면,\n",
    "* 즉, a x((b, c) x (d))가 되므로, 2D 텐서 X 1D 텐서 와 마찬가지로, c와 d는 동일해야 함\n",
    "* 즉, (a,b,c) x (c) 가 되고, 결과 shape는 동일한 c는 삭제되고, (a,b)가  됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((2, ), 2)\n",
    "A = torch.full((4, 3, 2), 3) # 3차원 tensor 생성\n",
    "result = torch.matmul(A, x) # 3차원 tensor와 vector의 행렬 곱 연산\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n - D텐서 (n > 2)간의 곱셈\n",
    "\n",
    "* 기본적으로 첫번째 인자와 두번째 인자의 마지막 두 axis 차원 이외의 동일함을 가정하고, Batched Matrix Multiplication 방식으로 계산됨\n",
    "  \n",
    "  * Batched Matrix Multiplication 방식이해 \n",
    "\n",
    "    * (b, m, n)은 (m, n) 행렬이 b개 있는 것이라고 볼 수 있음\n",
    "    * (b, m, n) x (b, n, k)는 b 개의 (m, n)을 b개의 (n, k)와 곱하는 것이라고 볼수 있음\n",
    "    * 따라서, tensor 간의 곱에서는 b자리는 동일해야 하며, (m, n), (n, k)는 행렬 곱셈과 동일한 제약사항을 가짐\n",
    "\n",
    "  * 첫번째 인자와 두번째 인자의 마지막 두 axis 차원 이외의 차원이 동일하지 않다면, broadcasting 룰이 적용됨\n",
    "\n",
    "    * 첫번째 인자의 마지막 두 axis 차원, 즉 matrix(m, n)과, 두번째 인자의 마지막 두 axis 차원, 즉 matrix (n, k) 사이에서만 곱하여 (m, k)으로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]]]) 4 torch.Size([10, 1, 3, 4])\n",
      "tensor([[[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]],\n",
      "\n",
      "        [[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]]]) 3 torch.Size([2, 4, 5])\n",
      "tensor([[[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]]]) 4 torch.Size([10, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "data1 = torch.full((10, 1, 3, 4), 2) # 4차원 tensor 생성\n",
    "data2 = torch.full((2,4,5), 3) # 3차원 tensor 생성\n",
    "\n",
    "print(data1, data1.dim(), data1.shape)\n",
    "print(data2, data2.dim(), data2.shape)\n",
    "data3 = torch.matmul(data1, data2) # 4차원 tensor와 3차원 tensor의 행렬 곱 연산\n",
    "print(data3, data3.dim(), data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]]]) 4 torch.Size([10, 8, 3, 4])\n",
      "tensor([[[[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]]]]) 4 torch.Size([1, 8, 4, 5])\n",
      "tensor([[[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]],\n",
      "\n",
      "\n",
      "        [[[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]],\n",
      "\n",
      "         [[24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24],\n",
      "          [24, 24, 24, 24, 24]]]]) 4 torch.Size([10, 8, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "data4 = torch.full((10, 8, 3, 4), 2) # 4차원 tensor 생성\n",
    "data5 = torch.full((1, 8, 4, 5), 3) # 4차원 tensor 생성\n",
    "\n",
    "print(data4, data4.dim(), data4.shape)\n",
    "print(data5, data5.dim(), data5.shape)\n",
    "\n",
    "data6 = torch.matmul(data4, data5) # 4차원 tensor와 4차원 tensor의 행렬 곱 연산\n",
    "print(data6, data6.dim(), data6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.mm()\n",
    "\n",
    "* 행렬곱만 지원\n",
    "* 두 인자 모두 행렬 shape 이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2, 2],\n",
      "        [2, 2, 2, 2],\n",
      "        [2, 2, 2, 2]]) 2 torch.Size([3, 4])\n",
      "tensor([[3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3]]) 2 torch.Size([4, 5])\n",
      "tensor([[24, 24, 24, 24, 24],\n",
      "        [24, 24, 24, 24, 24],\n",
      "        [24, 24, 24, 24, 24]]) 2 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "data7 = torch.full((3, 4), 2)\n",
    "data8 = torch.full((4, 5), 3)\n",
    "\n",
    "print(data7, data7.dim(), data7.shape)\n",
    "print(data8, data8.dim(), data8.shape)\n",
    "data9 = torch.matmul(data7, data8) # 2차원 tensor와 2차원 tensor의 행렬 곱 연산\n",
    "print(data9, data9.dim(), data9.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.bmm()\n",
    "\n",
    "* 두 인자 모두 3D 텐서 shape 이어야 함\n",
    "* broadcast를 지원하지 않으므로, 마지막 두 axis 외에는 두 인자 모두 동일한 차원이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 2, 2, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [2, 2, 2, 2]],\n",
      "\n",
      "        [[2, 2, 2, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [2, 2, 2, 2]],\n",
      "\n",
      "        [[2, 2, 2, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [2, 2, 2, 2]]]) 3 torch.Size([3, 3, 4])\n",
      "tensor([[[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]],\n",
      "\n",
      "        [[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]],\n",
      "\n",
      "        [[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]]]) 3 torch.Size([3, 4, 5])\n",
      "tensor([[[24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24]],\n",
      "\n",
      "        [[24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24]],\n",
      "\n",
      "        [[24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24],\n",
      "         [24, 24, 24, 24, 24]]]) 3 torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "data10 = torch.full((3,3,4), 2) # 3차원 tensor 생성\n",
    "data11 = torch.full((3,4,5), 3) # 3차원 tensor 생성\n",
    "\n",
    "print(data10, data10.dim(), data10.shape)\n",
    "print(data11, data11.dim(), data11.shape)\n",
    "\n",
    "data12 = torch.matmul(data10, data11) # 3차원 tensor와 3차원 tensor의 행렬 곱 연산\n",
    "print(data12, data12.dim(), data12.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
