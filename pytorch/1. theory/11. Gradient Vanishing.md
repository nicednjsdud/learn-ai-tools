
# Gradient Vanishing

* 다층 퍼셉트론에 오차역전파 적용시, 입력층으로 갈수록 기울기(Gradient)가 점차적으로 작아지는 현쌍

  * 반대로 입력층에 갈수록 기출기가 비정상적으로 커져서 발산하기도 함
  * 이는 Gradient Exploding 이라고 함

## Gradient Vanishing 이유

* activation function으로 적용한 sigmoid 함수부터 재정리 해보기로 함
* $y = \mathrm{sigmoid}(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}}$ (e는 자연상수로 2.718...)
* 결과 값을 0과 1사이의 값으로 조정하여 반환
  
  * 0 ~ 100%의 적절한 확률값을 가질 수 있고, 예측 결과가 선형회귀 모델보다 잘 매칭됨

* sigmoid 미분

  * 시그모이드 미분은 무조건 1보다 작음
  * $x < 1$ 이면, $x^10, x^100$과 같이 $x$를 곱할 수록, 최종값은 0에 수렴하게 됨

* activation function 미분 값이 1보다 작을 경우, 레이어가 입력에 가까울 수록, 해당 값이 0에 가까워질 수 있음
* 즉, sigmoid 이외에도, 적용한 activation function의 미분 값이 1보다 작은 경우, Gradient Vanishing 현상이 발생 가능
* 반대로, 만약 activation function의 미분값이 1보다 큰 경우, Gradient Exploding 현상이 발생할 수 있음

## Gradient Vanishing 이슈 해결 방안

* Gradient Vanishing 이슈를 해결하기 위해 고안된 activation function 이 ReLU (Rectified Linear Unit, 렐루) 함수임
* ReLU 함수

  * $y = ReLU(x) = max(0, x)$ 

* ReLU 함수의 미분은 $x >= 0$일 때는 1, $x < 0$일 때에는 0이므로, Gradient Vanishing 이슈를 어느 정도 보완할 수 있음
* $x >= 0$이면 미분이 1이므로, 학습 속도가 sigmoid 보다 빠른 편임

  * sigmoid 함수는 x 값이 커지면, y값이 1에 가까워지므로 기울기가 낮아져서
  * 일정 값 이상이 되면, 학습속도가 현저히 낮아짐
  