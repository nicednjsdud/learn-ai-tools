# Adaptive Gradient (AdaGrad)

* SGD, Momentum, NAG 모두 모든 파라미터 (예: $w_1, w_2, ..., w_n$ ) 업데이트에 동일한 learning rate 값 적용

  * 학습률이 너무 작으면, 학습 시간이 너무 길고, 학습률이 너무 크면, 발산해서 학습이 제대로 이루어지지 않음

* AdaGrad는 이를 학습률 감소 (learning rate decay)기법으로 개선하고자 함
  
  * iteration중, 많이 움직이는 경우(기울기가 컸던 경우), 해당 iteration의 learning rate는 그만큼 작아지므로, 학습률이 각 iteration마다 다르게 적용됨

* 단, Adagrad는 t가 증가함에 따라, $G_t$가 점점 커져서, learning rate가 점점 소실되는 문제점이 있음

## RMSProp (Root Mean square Propagation)

* Adagrad에서 발생한 t가 증가함에 따라 learning rate가 점점 소실되는 문제를 해결하고자 함
* G를 구할때, 합을 이용하지 않고, Exponential Weighted Moving Average (지수 가중 평균)를 이용
