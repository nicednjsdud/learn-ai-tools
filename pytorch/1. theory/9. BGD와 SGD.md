
# BGD 와 SGD

## 기본 Optimizer

* 딥러닝의 학습은 결국 손실값이 가장 작은 모델을 만드는 것임
* 즉, 학습은 손실함수(loss function) 최소값을 찾아가는 과정이고, 이 과정을 optimization(최적화) 이라고 함

  * 최적화를 수행하는 알고리즘을 Optimizer라고 함

* 대표적인 Optimizer가 Gradient Descent (경사 하강법)이며, 다양한 Optimizer가 제안되고 , 사용되고 잇음

## 주요 Gradient Descent Optimizer : SGD에 대한 이해

* Gradient Descent Optimizer 중 가장 많이 사용되는 기법이 SGD

## BGD와 SGD

### BGD

* 배치 경사 하강법 또는 Batch Gradient Descent라고 함

  * 전체 데이터셋에 대해 한번에 학습

### SGD

* 확률 경사 하강법 또는 Stochastic Gradient Descent 라고 함

  * 전체 학습 데이터 중 특정 크기만큼 임의로 선택해서 학습

### 미니 배치(Mini-Batch)

* epoch : 전체 데이터셋이 한번 최적화를 수행한 것
* iteration : 하나의 배치 (일정 크기의 데이터 사이즈)가 한번 최적화를 수행한 것
* 미니 배치(Mini-Batch)

  * 전체 데이터셋이 1000개가 있다고 할 때,
  * 이를 200개의 배치 사이즈로 분리하면, 각 200개의 서브 데이터셋을 미니배치라고 함
  * 1 epoch을 위해서 200개씩 5번 iteration이 필요함


## BGD (Batch Gradient Descent)

* 이 때의 Batch는 전체 데이터 셋을 의미함
* 전체 데이터셋을 기반으로 학습시키므로, 모델 파라미터 업데이트에 시간이 상대적으로 오래 걸림
* 전체 데이터셋으로 학습시키기 때문에, 수렴되는 정도가 일정한(안정적)
* 수렴되는 정도가 안정되기 때문에, local minima 상태가 되면 빠져나오기 어려움

## Local minima와 Global minima

* local minima 는 지역 최소점으로, 주변 cost 함수값 중에서 최소값을 가지는 지점을 의미함
* global minima 는 전역 최소점으로, 전체 cost 함수값이 최소값을 가지는 지점을 의미함

## SGD (Stochastic Gradient Descent)

* BGD의 단점을 보완하기 위해, 고안
  * 전체 데이터셋을 기반으로 학습시키므로, 모델 파라미터 업데이트에 시간이 상대적으로 오래 걸림
  * 수렴되는 정도가 안정되기 때문에, local minima 상태가 되면 빠져나오기 어려움
  * 전체 데이터셋이 아닌, 랜덤하게 선택된 k개의 샘플 데이터를 넣으면, 파라미터 업데이트 속도도 빨라지고, local minima를 빠져나올 수 있음
  
