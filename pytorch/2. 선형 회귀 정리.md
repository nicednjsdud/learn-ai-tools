
# 선형 회귀(Linear Regression)

시험 공부를 하는 시간이 길면, 성적이 잘 나온다.

* 특정 변수(정보)에 따라, 또 다른 특정 변수 값이 영향을 받음
* 성적을 변하게 하는 정보 요소들을 독립 변수(x)라고 하며, 독립 변수 값에 따라, 종속적으로 변하는 변수를 종속 변수(y)라고 함
* 선형 회귀는 한 개 이상의 독립 변수와 종속 변수간의 관계를 선형 모델로 구현
* **선형 : 1차원 함수 라고 생각하면 편함**

## 단순 선형 회귀 (simple linear regression)

* 독립 변수가 하나인 경우
* 단순 선형 회귀 수식 

$
y = wx + b
$

* 독립 변수 : x
* 가중치 w, 편향 : b

## 다중 선형 회귀 (multiple linear regression)

* 독립 변수가 여러 개인 경우
* 다중 선형 회귀 수식

$
y = w_2x_1 + w_2x_2 + w_nx_n + b
$

* 독립 변수 : x1 , x2 , xn
* 가중치 : w1, w2, wn , 편향 b

## 선형 회귀가 예측을 하는 방법 (예측 문제 분석)

### 예측 문제 분석

* 하루동안 평균 수학 공부 시간은 수학 성적에 영향을 주는 독립 변수, 수학 성적은 하루동안 평균 수학 공부 시간 독립 변수에 영향을 받는 종속 변수

| 하루동안 평균 수학 공부 시간 | 1시간 | 2시간 | 4시간 | 6시간 |
|-------|----|----|----|----|
| 수학성적 | 40점 | 60점 | 80점 | 90점 |

* 좌표 평면에 독립 변수와 종속 변수 간의 관계를 표시하고 보니, 선형적으로 표시될만한 관계가 보임
    * 따라서 선형 회귀로 문제를 풀기로 함
    * y = wx + b의 일차 함수 그래프로 표현 가능
        * 하루동안 평균 수학 공부 시간(독립 변수) x, 수학 성적(종속 변수)y
        * 적절한 가중치 w, 편향 b를 구하면 됨

* x와 y의 관계를 유추하기 위해 작성한 식을 **가설(Hypothesis)** 라고도 함 (H(x) 로도 표현함)

$
H(x) = wx + b
$


### 선형 회귀가 예측을 하는 방법 (오차 계산)

* 선형 회귀 가설식으로 예측된 값과, 실제 값 간의 차이를 가장 작게 만드는 적절한 가중치 w, 편향 b를 구하기 위해
* 선형 회귀 가설식으로 예측된 값과, 실제 값 간의 차이를 가장 잘 표현하는 오차 계산식을 정의해야 함
    * 일반 통계학, 그리고 이에 기반한 머신러닝에서는 표본으로부터 추정한 예측값과 실제값의 차이를 잔차(residual)라고도 함
    * 이를 비용 함수 (cost function), 손실 함수 (loss function), 또는 목적 함수 (objective function) 라고도 함.


### 비용함수도 다양한 정의가 사용될 수 있음

#### MAE(Mean Absolute Error) : 평균 절대 오차

* 실제 값과 예측 값의 차이의 절대값의 평균

$
MAE = \frac{ \sum_{i=0}^{N} |y_i - \hat{y}_i| }{N}
$

#### MSE(Mean Squared Error) : 평균 제곱근 오차

* 실제 값과 예측 값의 차이를 제곱한 값의 평균

$
MSE = \frac{ \sum_{i=0}^{N} (y_i - \hat{y}_i)^2 }{N}
$

#### RMSE(Root Mean Squared Error) : 평균 제곱근 편차

* MSE 값은 제곱 값이므로, 실제 오류 평균보다 값이 커지는 특성이 있으므로, 이를 줄이기 위해 MSE에 루트를 씌운 지표

$
RMSE = \sqrt{ \frac{ \sum_{i=0}^{N} (y_i - \hat{y}_i)^2 }{N} }
$



#### RMSLE(Root Mean Squared Log Error) : 평균 제곱근 대수 오차

* RMSE에 log를 적용해준 지표


$
RMSLE = \sqrt{ 
\frac{
\sum_{i=0}^{N}
\left( \log(y_i + 1) - \log(\hat{y}_i + 1) \right)^2
}{N}
}
$


```
  회귀 문제에서는 주소 MSE를 많이 사용함 (미분 계산에 용이하기 때문)
```

### 선형 회귀가 예측을 하는 방법 (오차를 최소화 하는 방법)

$H(x) = wx + b = \hat{y} : 예측값('y 햇' 이라고 읽음)$

* y: 실제 값
* **MSE**로 오차를 계산하는 식을 만든다면,

$\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = cost(w, b)$

* 즉, cost(w,b)를 최소화하는 w와 b를 구하면 됨.

### 최소 제곱법

$\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = cost(w, b)$

* 최소가 되기 위해서는 w,b를 편미분했을 때의 값이 0이면 됨

$\frac{\partial}{\partial a} \sum_{i=1}^{n} \left( y_i - (w x_i + b) \right)^2 = 0$

$\frac{\partial}{\partial b} \sum_{i=1}^{n} \left( y_i - (w x_i + b) \right)^2 = 0$


$cost(w, b) = (y_i - (w x_i + b))^2$

```
최소 제곱번은 여러개의 독립 변수가 있을 때에는 계산이 어여룸
특히, 딥러닝/머신러닝 문제는 대부분 독립변수가 매우 많기 때문에, 다른 방법이 필요했음
```

### 경사 하강법 (Gradient Descent)

$\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = cost(w, b)$

* y식에 다양한 독립 변수가 있는 경우에도 적용 가능한 기법
* 다 변수 함수(독립 변수가 둘 이상인 함수)에서 최소값을 찾기 위해, 가중치 값을 업데이트 하는 방법

* 결국 y식이 어떻든 간에, 비용함수를 최소하하는 w와 b를 찾는 것이고,
* 비용함수는 제곱의 형태를 띄고 있으므로, 편미분을 기반으로 계산

$w := w - \alpha \frac{\partial}{\partial w} cost(w, b)$

$b := b - \alpha \frac{\partial}{\partial b} cost(w, b)$

$\alpha$ : Learning rate (하이퍼 파라미터, 임의로 설정해줘야 하는 값, 주로 0.01, 0.05, 0.1 등)

* 다음과 같이 미분값(기울기값)이 +면, $w$는 더 낮은 값으로, 미분값(기울기값)이 -면, $w$는 더 큰값으로 계산되며,
* 기울기 값이 0에 근접한 값이 될때까지 반복 (또는 일정 횟수만큼 반복하기도 함)

* $(y - f_i(x))^2$를 비용함수로 가정하면, 미분을 통해 순간 기울기를 계산하고, learning rate만큼 반영하여, 조금씩 최소값을 찾아가는 형태

* Learning rate를 너무 높게 잡으면, 최소값에 근접하지 못하고, 발산할 수도 있고, 너무 낮게 잡으면, 최소값에 근접하기 전에 끝날 수도 있음
    * 일반적으로 테스트를 통해, 적절한 learning rate를 설정하고 있음

* 다중 선형 회귀의 경우에도 각 $w_i$ 기울기를 경사하강법으로 계산 가능함

* $y = w_1x_1 + w_2x_2 ... + w_nx_n + b$
* $b_1, b_2, ..., b_n$은 결국 더하면 하나의 $b$로 표현가능
* 각 $w_i$별로 경사 하강법 적용 가능
    * $w_1 := w_1 - \alpha \frac{\partial}{\partial w_1} cost(w, b)$
    * $w_2 := w_2 - \alpha \frac{\partial}{\partial w_2} cost(w, b)$
    * $\vdots$
    * $w_n := w_n - \alpha \frac{\partial}{\partial w_n} cost(w, b)$
    * $b := b - \alpha \frac{\partial}{\partial b} cost(w, b)$